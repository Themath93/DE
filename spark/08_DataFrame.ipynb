{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "503abec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b998ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, datetime\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525a7bed",
   "metadata": {},
   "source": [
    "# 1. DataFrame 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7216d168",
   "metadata": {},
   "source": [
    "- SparkSession 객체를 사용해 DataFrame을 생성할 수 있다.\n",
    "- SparkSession 객체는 pyspark shell을 실행할 때 spark 라는 이름으로 미리 생성된다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7852f6",
   "metadata": {},
   "source": [
    "## Row 객체를 사용해 생성하기\n",
    "\n",
    "- row : DataFrame에서의 한 행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f722abe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.4.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.18.5; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from pandas) (1.23.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas) (1.14.0)\n",
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.8/dist-packages (8.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.8/dist-packages (from pyarrow) (1.23.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+\n",
      "|  name|age|     birth|\n",
      "+------+---+----------+\n",
      "|하명도| 15|2022-07-22|\n",
      "|이제동| 20|2021-07-22|\n",
      "|김명운| 25|2020-07-22|\n",
      "+------+---+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# !pip install pandas\n",
    "# !pip install pyarrow\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import date, datetime\n",
    "from pyspark.sql import *\n",
    "\n",
    "# Row class의 생성자로 keyword args를 전달해 생성\n",
    "df = spark.createDataFrame([\n",
    "    Row(name='하명도', age=15, birth=date(2022,7,22)),\n",
    "    Row(name='이제동', age=20, birth=date(2021,7,22)),\n",
    "    Row(name='김명운', age=25, birth=date(2020,7,22))\n",
    "])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb710e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- birth: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 스키마 확인\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca54c584",
   "metadata": {},
   "source": [
    "## schema를 명시하여 DataFrame 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2295388",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+\n",
      "|  name|age|     birth|\n",
      "+------+---+----------+\n",
      "|하명도| 15|2022-07-22|\n",
      "|이제동| 20|2021-07-22|\n",
      "|김명운| 25|2020-07-22|\n",
      "+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 튜플에 데이터를 저장하고 스키마를 직접 지정\n",
    "\n",
    "df2 = spark.createDataFrame([\n",
    "    ('하명도', 15, date(2022,7,22)),\n",
    "    ('이제동', 20, date(2021,7,22)),\n",
    "    ('김명운', 25, date(2020,7,22))\n",
    "],schema='name string, age int, birth date')\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523fee7a",
   "metadata": {},
   "source": [
    "## StructType 객체를 사용해 Schema 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08545ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = false)\n",
      " |-- age: integer (nullable = false)\n",
      " |-- birth: date (nullable = false)\n",
      "\n",
      "+------+---+----------+\n",
      "|  name|age|     birth|\n",
      "+------+---+----------+\n",
      "|하명도| 15|2022-07-22|\n",
      "|이제동| 20|2021-07-22|\n",
      "|김명운| 25|2020-07-22|\n",
      "+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "data = [\n",
    "    ('하명도', 15, date(2022,7,22)),\n",
    "    ('이제동', 20, date(2021,7,22)),\n",
    "    ('김명운', 25, date(2020,7,22))\n",
    "]\n",
    "\n",
    "# 자바 문서나 스칼라 문서를 찾아볼 것\n",
    "# 파이썬 문서는 상속계층이나 패키지별로 정리가 안되어있고 내용별로 정리되어 있어서 사람이 볼 것이 못 된다...\n",
    "schema = StructType([\n",
    "    StructField('name', StringType(), False),\n",
    "    StructField('age', IntegerType(), False),\n",
    "    StructField('birth', DateType(), False)\n",
    "])\n",
    "\n",
    "df3 = spark.createDataFrame(data=data, schema=schema)\n",
    "df3.printSchema()\n",
    "df3.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0399d3a",
   "metadata": {},
   "source": [
    "## 중첩스키마적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fdc8beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = false)\n",
      " |-- age: integer (nullable = false)\n",
      " |-- birth: date (nullable = false)\n",
      " |-- phone: struct (nullable = false)\n",
      " |    |-- phone1: string (nullable = true)\n",
      " |    |-- phone2: string (nullable = true)\n",
      " |    |-- phone3: string (nullable = true)\n",
      "\n",
      "+------+---+----------+-----------------+\n",
      "|  name|age|     birth|            phone|\n",
      "+------+---+----------+-----------------+\n",
      "|하명도| 15|2022-07-22|{010, 1111, 2222}|\n",
      "|이제동| 20|2021-07-22|{010, 2222, 3333}|\n",
      "|김명운| 25|2020-07-22|{010, 4444, 5555}|\n",
      "+------+---+----------+-----------------+\n",
      "\n",
      "{\"fields\":[{\"metadata\":{\"desc\":\"\\uc774\\ub984\"},\"name\":\"name\",\"nullable\":false,\"type\":\"string\"},{\"metadata\":{\"desc\":\"\\ub098\\uc774\"},\"name\":\"age\",\"nullable\":false,\"type\":\"integer\"},{\"metadata\":{\"desc\":\"\\uc0dd\\uc77c\"},\"name\":\"birth\",\"nullable\":false,\"type\":\"date\"},{\"metadata\":{\"desc\":\"\\uc804\\ud654\\ubc88\\ud638\"},\"name\":\"phone\",\"nullable\":false,\"type\":{\"fields\":[{\"metadata\":{},\"name\":\"phone1\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"phone2\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"phone3\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"}}],\"type\":\"struct\"}\n",
      "-----------------------------------------------------\n",
      "{\"fields\":[{\"metadata\":{\"desc\":\"이름\"},\"name\":\"name\",\"nullable\":false,\"type\":\"string\"},{\"metadata\":{\"desc\":\"나이\"},\"name\":\"age\",\"nullable\":false,\"type\":\"integer\"},{\"metadata\":{\"desc\":\"생일\"},\"name\":\"birth\",\"nullable\":false,\"type\":\"date\"},{\"metadata\":{\"desc\":\"전화번호\"},\"name\":\"phone\",\"nullable\":false,\"type\":{\"fields\":[{\"metadata\":{},\"name\":\"phone1\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"phone2\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"phone3\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"}}],\"type\":\"struct\"}\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    ('하명도', 15, date(2022,7,22), ('010','1111','2222')),\n",
    "    ('이제동', 20, date(2021,7,22), ('010','2222','3333')),\n",
    "    ('김명운', 25, date(2020,7,22), ('010','4444','5555'))\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('name', StringType(), False, {\"desc\":\"이름\"}),\n",
    "    StructField('age', IntegerType(), False, {\"desc\":\"나이\"}),\n",
    "    StructField('birth', DateType(), False, {\"desc\":\"생일\"}),\n",
    "    StructField('phone', StructType([\n",
    "        StructField('phone1', StringType(), True),\n",
    "        StructField('phone2', StringType(), True),\n",
    "        StructField('phone3', StringType(), True),\n",
    "    \n",
    "    ]), False, {\"desc\":\"전화번호\"})\n",
    "])\n",
    "\n",
    "df4 = spark.createDataFrame(data=data, schema=schema)\n",
    "df4.printSchema()\n",
    "df4.show()\n",
    "\n",
    "# 스키마를 json으로 변환하여 확인\n",
    "schema_json = df4.schema.json()\n",
    "\n",
    "# 이름 => \\uc774\\ub984  유니코드로 나온다\n",
    "print(schema_json)\n",
    "\n",
    "\n",
    "print('-----------------------------------------------------')\n",
    "\n",
    "# 바이트코드로 변환한 뒤 다시 문자열 디코딩을 할 때 unicode_escape 옵션을 추가\n",
    "schema_json = schema_json.encode().decode('unicode_escape')\n",
    "print(schema_json)                                          \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8485e30",
   "metadata": {},
   "source": [
    "## Pandas DataFrame으로 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e443c4f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+\n",
      "|  name|age|     birth|\n",
      "+------+---+----------+\n",
      "|하명도| 20|2022-07-01|\n",
      "|이제동| 21|2022-07-02|\n",
      "|김명운| 22|2022-07-03|\n",
      "+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pandas_df = pd.DataFrame({\n",
    "    'name':['하명도','이제동','김명운'],\n",
    "    'age':[20, 21, 22],\n",
    "    'birth':[date(2022,7,1),date(2022,7,2),date(2022,7,3)]\n",
    "})\n",
    "\n",
    "type(pandas_df)\n",
    "\n",
    "df3 = spark.createDataFrame(pandas_df)\n",
    "df3.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bec6a6f",
   "metadata": {},
   "source": [
    "## DataFrame -> Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2853cec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>birth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>하명도</td>\n",
       "      <td>20</td>\n",
       "      <td>2022-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>이제동</td>\n",
       "      <td>21</td>\n",
       "      <td>2022-07-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>김명운</td>\n",
       "      <td>22</td>\n",
       "      <td>2022-07-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  name  age       birth\n",
       "0  하명도   20  2022-07-01\n",
       "1  이제동   21  2022-07-02\n",
       "2  김명운   22  2022-07-03"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df2 = df3.toPandas()\n",
    "pandas_df2\n",
    "\n",
    "# 스파크의 DataFrame을 사용하는 것이 성능상 더 이득\n",
    "# 스파크는 병렬처리도 해주고... 쿼리실행 최적화도 해주고...\n",
    "# 하지만 스파크 api가 Pandas에 비해 제공되는 기능이 적어서\n",
    "# Pandas를 써야만 해결이 가능하다면 Pandas로 가공 이후 스파크 DataFrame으로 변환도 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1729990c",
   "metadata": {},
   "source": [
    "## DataFrame -> pyspark.pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b7d832a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>birth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>하명도</td>\n",
       "      <td>20</td>\n",
       "      <td>2022-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>이제동</td>\n",
       "      <td>21</td>\n",
       "      <td>2022-07-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>김명운</td>\n",
       "      <td>22</td>\n",
       "      <td>2022-07-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  name  age       birth\n",
       "0  하명도   20  2022-07-01\n",
       "1  이제동   21  2022-07-02\n",
       "2  김명운   22  2022-07-03"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install pyarrow\n",
    "df3.to_pandas_on_spark()\n",
    "type(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccba3fc1",
   "metadata": {},
   "source": [
    "## 외부파일을 사용해 DataFrame 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bed5830f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-------------+--------+-----------+-------------+\n",
      "|class_cd|school|class_std_cnt|     loc|school_type|teaching_type|\n",
      "+--------+------+-------------+--------+-----------+-------------+\n",
      "|     6OL| ANKYI|           20|   Urban| Non-public|     Standard|\n",
      "|     ZNS| ANKYI|           21|   Urban| Non-public|     Standard|\n",
      "|     2B1| CCAAW|           18|Suburban| Non-public| Experimental|\n",
      "|     EPS| CCAAW|           20|Suburban| Non-public| Experimental|\n",
      "|     IQN| CCAAW|           15|Suburban| Non-public| Experimental|\n",
      "+--------+------+-------------+--------+-----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "-RECORD 0-------------------\n",
      " class_cd      | 6OL        \n",
      " school        | ANKYI      \n",
      " class_std_cnt | 20         \n",
      " loc           | Urban      \n",
      " school_type   | Non-public \n",
      " teaching_type | Standard   \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_df = spark.read.csv('/dataframe/a_class_info.csv', header=True)\n",
    "class_df.show(5)\n",
    "class_df.show(1, vertical=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4eb39b6",
   "metadata": {},
   "source": [
    "## DataFrame 컬럼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c56474",
   "metadata": {},
   "source": [
    "- withColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "096328b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    ('하명도', 15, date(2022,7,22), ('010','1111','2222')),\n",
    "    ('이제동', 20, date(2021,7,22), ('010','2222','3333')),\n",
    "    ('김명운', 25, date(2020,7,22), ('010','4444','5555')),\n",
    "    ('홍진호', 36, date(2018,7,22), ('010','3333','4444'))\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('name', StringType(), False, {\"desc\":\"이름\"}),\n",
    "    StructField('age', IntegerType(), False, {\"desc\":\"나이\"}),\n",
    "    StructField('birth', DateType(), False, {\"desc\":\"생일\"}),\n",
    "    StructField('phone', StructType([\n",
    "        StructField('phone1', StringType(), True),\n",
    "        StructField('phone2', StringType(), True),\n",
    "        StructField('phone3', StringType(), True),\n",
    "    \n",
    "    ]), False, {\"desc\":\"전화번호\"})\n",
    "])\n",
    "\n",
    "col_df = spark.createDataFrame(data, schema=schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "703154c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+-----------------+--------+\n",
      "|  name|age|     birth|            phone|우승여부|\n",
      "+------+---+----------+-----------------+--------+\n",
      "|하명도| 15|2022-07-22|{010, 1111, 2222}|        |\n",
      "|이제동| 20|2021-07-22|{010, 2222, 3333}|        |\n",
      "|김명운| 25|2020-07-22|{010, 4444, 5555}|        |\n",
      "|홍진호| 36|2018-07-22|{010, 3333, 4444}|        |\n",
      "+------+---+----------+-----------------+--------+\n",
      "\n",
      "+------+---+----------+-----------------+--------+\n",
      "|  name|age|     birth|            phone|우승여부|\n",
      "+------+---+----------+-----------------+--------+\n",
      "|하명도| 15|2022-07-22|{010, 1111, 2222}|    우승|\n",
      "|이제동| 20|2021-07-22|{010, 2222, 3333}|    우승|\n",
      "|김명운| 25|2020-07-22|{010, 4444, 5555}|    우승|\n",
      "|홍진호| 36|2018-07-22|{010, 3333, 4444}|    우승|\n",
      "+------+---+----------+-----------------+--------+\n",
      "\n",
      "+------+---+----------+-----------------+---------+\n",
      "|  name|age|     birth|            phone|   연령대|\n",
      "+------+---+----------+-----------------+---------+\n",
      "|하명도| 15|2022-07-22|{010, 1111, 2222}|     10대|\n",
      "|이제동| 20|2021-07-22|{010, 2222, 3333}|     20대|\n",
      "|김명운| 25|2020-07-22|{010, 4444, 5555}|     20대|\n",
      "|홍진호| 36|2018-07-22|{010, 3333, 4444}|30대 이상|\n",
      "+------+---+----------+-----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# withColumn : 컬럼이름, 컬럼\n",
    "# lit : column 객체를 literal로 만들어주는 함수\n",
    "\n",
    "# 원하는 컬럼을 DataFrame에 추가\n",
    "col_df.withColumn('우승여부', lit('')).show()\n",
    "\n",
    "# 값을 지정해서 추가\n",
    "col_df.withColumn('우승여부', lit('우승')).show()\n",
    "\n",
    "# when - otherwise : 조건에 따라 원하는 컬럼객체를 반환\n",
    "temp = col_df.withColumn('연령대', when(floor(col_df.age / 10) == 1, '10대')\n",
    "                           .when(floor(col_df.age / 10) == 2, '20대')\n",
    "                           .otherwise('30대 이상')\n",
    "                 )\n",
    "\n",
    "temp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b36e25",
   "metadata": {},
   "source": [
    "### column  내용  변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "07f30739",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+-----------------+------+\n",
      "|  name|age|     birth|            phone|연령대|\n",
      "+------+---+----------+-----------------+------+\n",
      "|하명도| 15|2022-07-22|{010, 1111, 2222}|어린이|\n",
      "|이제동| 20|2021-07-22|{010, 2222, 3333}|  청년|\n",
      "|김명운| 25|2020-07-22|{010, 4444, 5555}|  청년|\n",
      "|홍진호| 36|2018-07-22|{010, 3333, 4444}|  성인|\n",
      "+------+---+----------+-----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# when - otherwise : 조건에 따라 원하는 컬럼객체를 반환\n",
    "temp = temp.withColumn('연령대', when(floor(col_df.age / 10) == 1, '어린이')\n",
    "                           .when(floor(col_df.age / 10) == 2, '청년')\n",
    "                           .when(floor(col_df.age / 10) == 3, '성인')\n",
    "                 )\n",
    "\n",
    "temp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c4ae4f",
   "metadata": {},
   "source": [
    "### column 이름 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c7337f18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+-----------------+------+\n",
      "|  name|age|     birth|            phone|  분류|\n",
      "+------+---+----------+-----------------+------+\n",
      "|하명도| 15|2022-07-22|{010, 1111, 2222}|어린이|\n",
      "|이제동| 20|2021-07-22|{010, 2222, 3333}|  청년|\n",
      "|김명운| 25|2020-07-22|{010, 4444, 5555}|  청년|\n",
      "|홍진호| 36|2018-07-22|{010, 3333, 4444}|  성인|\n",
      "+------+---+----------+-----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp = temp.withColumnRenamed('연령대','분류')\n",
    "temp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d077b939",
   "metadata": {},
   "source": [
    "### column  삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f4f1ac71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+-----------------+\n",
      "|  name|age|     birth|            phone|\n",
      "+------+---+----------+-----------------+\n",
      "|하명도| 15|2022-07-22|{010, 1111, 2222}|\n",
      "|이제동| 20|2021-07-22|{010, 2222, 3333}|\n",
      "|김명운| 25|2020-07-22|{010, 4444, 5555}|\n",
      "|홍진호| 36|2018-07-22|{010, 3333, 4444}|\n",
      "+------+---+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp = temp.drop('분류')\n",
    "temp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aedcf61",
   "metadata": {},
   "source": [
    "# 2. DataFrame 사용 하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b533d543",
   "metadata": {},
   "source": [
    "참고 : https://spark.apache.org/docs/3.2.0/api/scala/org/apache/spark/sql/Dataset.html \n",
    "\n",
    "- DataFrame의 메서드의 구분\n",
    " - transformation\n",
    " - action\n",
    " - Basic Dataset functions  \n",
    " \n",
    " \n",
    "- DataFrame의 사용은 SQL 쿼리 구조를 따라간다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
